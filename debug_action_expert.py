#!/usr/bin/env python3
"""
ç‹¬ç«‹è°ƒè¯• Action Expert çš„æµ‹è¯•è„šæœ¬

è¿™ä¸ªè„šæœ¬å…è®¸ä½ ç‹¬ç«‹æµ‹è¯•å’Œä¿®æ”¹ action expertï¼Œè€Œä¸éœ€è¦å®Œæ•´çš„ PI0 æ¨¡å‹ã€‚
"""

import torch
from torch import nn
from transformers import GemmaForCausalLM
from transformers.models.auto import CONFIG_MAPPING

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

import openpi.models.gemma as _gemma


class ActionExpertDebugger:
    def __init__(self, variant="dummy", precision="float32", use_adarms=False):
        """
        åˆå§‹åŒ– Action Expert è°ƒè¯•å™¨
        
        Args:
            variant: gemma å˜ä½“ ("dummy", "gemma_300m", "gemma_2b" ç­‰)
            precision: ç²¾åº¦ ("float32" or "bfloat16") 
            use_adarms: æ˜¯å¦ä½¿ç”¨ adaptive RMS normalization
        """
        self.variant = variant
        self.precision = precision
        self.use_adarms = use_adarms
        
        print(f"åˆå§‹åŒ– Action Expert: {variant}, ç²¾åº¦: {precision}, AdaRMS: {use_adarms}")
        
        # è·å–é…ç½®
        self.config = _gemma.get_config(variant)
        print(f"é…ç½®: {self.config}")
        
        # åˆ›å»º HuggingFace é…ç½®
        self.action_expert_config_hf = CONFIG_MAPPING["gemma"](
            head_dim=self.config.head_dim,
            hidden_size=self.config.width,
            intermediate_size=self.config.mlp_dim,
            num_attention_heads=self.config.num_heads,
            num_hidden_layers=self.config.depth,
            num_key_value_heads=self.config.num_kv_heads,
            vocab_size=257152,  # PALIGEMMA_VOCAB_SIZE
            hidden_activation="gelu_pytorch_tanh",
            torch_dtype=precision,
            use_adarms=use_adarms,
            adarms_cond_dim=self.config.width if use_adarms else None,
        )
        
        # åˆ›å»ºæ¨¡å‹
        self.action_expert = GemmaForCausalLM(config=self.action_expert_config_hf)
        self.action_expert.model.embed_tokens = None  # ç§»é™¤ embedding å±‚
        
        # è®¾ç½®ç²¾åº¦
        if precision == "bfloat16":
            self.action_expert = self.action_expert.to(dtype=torch.bfloat16)
        
        print(f"Action Expert å‚æ•°æ•°é‡: {sum(p.numel() for p in self.action_expert.parameters())}")
        
    def create_mock_inputs(self, batch_size=2, seq_len=8, device="cpu"):
        """åˆ›å»ºæµ‹è¯•ç”¨çš„ mock è¾“å…¥æ•°æ®"""
        # åˆ›å»ºåµŒå…¥è¾“å…¥ (è€Œä¸æ˜¯ token ids)
        inputs_embeds = torch.randn(
            batch_size, seq_len, self.config.width,
            dtype=torch.float32 if self.precision == "float32" else torch.bfloat16,
            device=device
        )
        
        # åˆ›å»ºæ³¨æ„åŠ›æ©ç 
        attention_mask = torch.ones(batch_size, seq_len, dtype=torch.bool, device=device)
        
        # åˆ›å»ºä½ç½® ids
        position_ids = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, -1)
        
        # å¦‚æœä½¿ç”¨ adaRMSï¼Œåˆ›å»ºæ¡ä»¶è¾“å…¥
        adarms_cond = None
        if self.use_adarms:
            adarms_cond = torch.randn(batch_size, self.config.width, device=device)
        
        return {
            "inputs_embeds": inputs_embeds,
            "attention_mask": attention_mask, 
            "position_ids": position_ids,
            "adarms_cond": adarms_cond
        }
    
    def test_forward_pass(self, batch_size=2, seq_len=8, device="cpu"):
        """æµ‹è¯•å‰å‘ä¼ æ’­"""
        print(f"\n=== æµ‹è¯•å‰å‘ä¼ æ’­ (batch_size={batch_size}, seq_len={seq_len}) ===")
        
        # åˆ›å»ºè¾“å…¥
        inputs = self.create_mock_inputs(batch_size, seq_len, device)
        
        print(f"è¾“å…¥å½¢çŠ¶: {inputs['inputs_embeds'].shape}")
        print(f"è¾“å…¥æ•°æ®ç±»å‹: {inputs['inputs_embeds'].dtype}")
        
        # å‰å‘ä¼ æ’­
        try:
            with torch.no_grad():
                outputs = self.action_expert.model.forward(
                    inputs_embeds=inputs["inputs_embeds"],
                    attention_mask=inputs["attention_mask"],
                    position_ids=inputs["position_ids"],
                    adarms_cond=inputs["adarms_cond"],
                    use_cache=False
                )
            
            hidden_states = outputs.last_hidden_state
            print(f"è¾“å‡ºå½¢çŠ¶: {hidden_states.shape}")
            print(f"è¾“å‡ºæ•°æ®ç±»å‹: {hidden_states.dtype}")
            print(f"è¾“å‡ºæ•°å€¼èŒƒå›´: [{hidden_states.min():.4f}, {hidden_states.max():.4f}]")
            print("âœ… å‰å‘ä¼ æ’­æˆåŠŸ!")
            
            return outputs
            
        except Exception as e:
            print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
            return None
    
    def test_gradient_flow(self, batch_size=2, seq_len=8, device="cpu"):
        """æµ‹è¯•æ¢¯åº¦æµ"""
        print(f"\n=== æµ‹è¯•æ¢¯åº¦æµ ===")
        
        # åˆ›å»ºè¾“å…¥ï¼Œéœ€è¦æ¢¯åº¦
        inputs = self.create_mock_inputs(batch_size, seq_len, device)
        inputs["inputs_embeds"].requires_grad_(True)
        
        try:
            # å‰å‘ä¼ æ’­
            outputs = self.action_expert.model.forward(
                inputs_embeds=inputs["inputs_embeds"],
                attention_mask=inputs["attention_mask"], 
                position_ids=inputs["position_ids"],
                adarms_cond=inputs["adarms_cond"],
                use_cache=False
            )
            
            # è®¡ç®—æŸå¤±
            hidden_states = outputs.last_hidden_state
            loss = hidden_states.mean()
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ£€æŸ¥æ¢¯åº¦
            grad_norm = inputs["inputs_embeds"].grad.norm()
            print(f"è¾“å…¥æ¢¯åº¦èŒƒæ•°: {grad_norm:.6f}")
            
            # æ£€æŸ¥å‚æ•°æ¢¯åº¦
            param_grad_norms = []
            for name, param in self.action_expert.named_parameters():
                if param.grad is not None:
                    param_grad_norms.append((name, param.grad.norm().item()))
            
            print(f"å‚æ•°æ¢¯åº¦æ•°é‡: {len(param_grad_norms)}")
            if param_grad_norms:
                max_grad = max(param_grad_norms, key=lambda x: x[1])
                print(f"æœ€å¤§å‚æ•°æ¢¯åº¦: {max_grad[0]} = {max_grad[1]:.6f}")
            
            print("âœ… æ¢¯åº¦æµæµ‹è¯•æˆåŠŸ!")
            
        except Exception as e:
            print(f"âŒ æ¢¯åº¦æµæµ‹è¯•å¤±è´¥: {e}")
    
    def benchmark_speed(self, batch_size=2, seq_len=8, num_runs=10, device="cpu"):
        """åŸºå‡†æµ‹è¯•é€Ÿåº¦"""
        print(f"\n=== é€Ÿåº¦åŸºå‡†æµ‹è¯• (runs={num_runs}) ===")
        
        inputs = self.create_mock_inputs(batch_size, seq_len, device)
        
        # é¢„çƒ­
        with torch.no_grad():
            for _ in range(3):
                self.action_expert.model.forward(**inputs, use_cache=False)
        
        # è®¡æ—¶
        import time
        start_time = time.time()
        
        with torch.no_grad():
            for _ in range(num_runs):
                self.action_expert.model.forward(**inputs, use_cache=False)
        
        end_time = time.time()
        avg_time = (end_time - start_time) / num_runs
        
        print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_time*1000:.2f} ms")
        print(f"ååé‡: {batch_size/avg_time:.2f} samples/sec")


def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸ” Action Expert ç‹¬ç«‹è°ƒè¯•æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•ä¸åŒé…ç½®
    configs_to_test = [
        {"variant": "dummy", "precision": "float32", "use_adarms": False},
        {"variant": "dummy", "precision": "float32", "use_adarms": True},
        # {"variant": "gemma_300m", "precision": "float32", "use_adarms": False},  # è§£æ³¨é‡Šä»¥æµ‹è¯•æ›´å¤§æ¨¡å‹
    ]
    
    for config in configs_to_test:
        print(f"\nğŸ§ª æµ‹è¯•é…ç½®: {config}")
        print("-" * 40)
        
        try:
            debugger = ActionExpertDebugger(**config)
            
            # è¿è¡Œæµ‹è¯•
            debugger.test_forward_pass()
            debugger.test_gradient_flow() 
            debugger.benchmark_speed()
            
        except Exception as e:
            print(f"âŒ é…ç½®æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"\nâœ¨ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    main()