# Self-Alignment VLA Implementation Plan

**åˆ›å»ºæ—¶é—´**: 2025-10-14
**ç›®æ ‡**: å®ç°åŸºäºè‡ªå¯¹é½çš„é›¶æ ·æœ¬è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹

## æ ¸å¿ƒæ€æƒ³

é€šè¿‡è§£è€¦ä¸¤ç§è¡¨å¾å®ç°é›¶æ ·æœ¬å­¦ä¹ ï¼š
1. **Embodiment-agnosticï¼ˆä¸å…·èº«æ— å…³ï¼‰è¡¨å¾**: é€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒVLAå­¦ä¹ 
2. **Embodiment-relevantï¼ˆä¸å…·èº«ç›¸å…³ï¼‰è¡¨å¾**: ç¼–ç åœ¨TTTå±‚å‚æ•°Wä¸­ï¼Œé€šè¿‡play dataè‡ªå¯¹é½

å…³é”®åˆ›æ–°ï¼š
- TTTå±‚çš„å‚æ•° (W1, b1) å°±æ˜¯ embodiment context W
- Alignment experts æä¾›è‡ªç›‘ç£ä¿¡å·ï¼Œä¸éœ€è¦æ ‡æ³¨
- å¯¹æ¯”å­¦ä¹ åˆ†ç¦»ä¸¤ç§è¡¨å¾

---

## å·²å®Œæˆå·¥ä½œ âœ…

### 1. åŸºç¡€æ¶æ„æ­å»º (2025-10-14)

#### 1.1 ä¿®æ”¹ `PaliGemmaWithExpertModel` (gemma_pytorch.py)
- âœ… æ·»åŠ  `alignment_expert_config` å‚æ•°
- âœ… æ·»åŠ  `use_alignment_expert` å‚æ•°
- âœ… å®ä¾‹åŒ– `self.alignment_expert = GemmaForCausalLM(config=alignment_expert_config_hf)`
- âœ… ä½¿ç”¨ `gemma_300m` ä½œä¸ºè½»é‡çº§æ¶æ„ï¼ˆ18å±‚ï¼Œ311Må‚æ•°ï¼‰
- âœ… Alignment expert ä¸ä½¿ç”¨ TTTï¼ˆæ¥æ”¶ Action Expert çš„ TTT è¾“å‡ºï¼‰
- âœ… æ‰©å±• `use_adarms` ä¸º `[VLM, Action Expert, Alignment Expert]`

#### 1.2 ä¿®æ”¹ `PI0Pytorch` (pi0_pytorch.py)
- âœ… æ·»åŠ  `alignment_expert_config` åŠ è½½é€»è¾‘
- âœ… ä¿®å¤ `use_adarms` bug (line 107)
- âœ… ä¼ é€’é…ç½®åˆ° `PaliGemmaWithExpertModel`
- âœ… æ·»åŠ ä¸‰ä¸ª prediction heads:
  ```python
  self.inverse_dynamics_head = nn.Linear(config.width, 32)  # -> action_dim
  self.dynamics_head = nn.Linear(config.width, config.width)  # -> obs features
  self.perception_head = nn.Linear(config.width, 32)  # -> state_dim
  ```

#### 1.3 ä¿®æ”¹é…ç½® (pi0_config.py)
- âœ… æ·»åŠ  `alignment_expert_variant: _gemma.Variant = "gemma_300m"`
- âœ… å·²æœ‰ `use_alignment_expert: bool = False`

### å½“å‰æ¶æ„

```
PaliGemmaWithExpertModel:
â”œâ”€â”€ self.paligemma          # VLM (gemma_2b, 18å±‚)
â”œâ”€â”€ self.gemma_expert       # Action Expert (gemma_300m, 18å±‚ + TTT)
â””â”€â”€ self.alignment_expert   # Alignment Expert (gemma_300m, 18å±‚, no TTT)

PI0Pytorch:
â”œâ”€â”€ self.paligemma_with_expert
â”œâ”€â”€ self.action_in_proj / self.action_out_proj
â””â”€â”€ Alignment heads (when use_alignment_expert=True):
    â”œâ”€â”€ self.inverse_dynamics_head  # (obs_t, obs_t+1) -> action_t
    â”œâ”€â”€ self.dynamics_head          # (obs_t, action_t) -> obs_t+1
    â””â”€â”€ self.perception_head        # obs_t -> state_t
```

---

## Phase 1: å®ŒæˆåŸºç¡€æ¶æ„ [P0] (1-2å¤©)

### 1.1 æµ‹è¯•æ¨¡å‹åˆå§‹åŒ– â³
**æ–‡ä»¶**: åˆ›å»º `tests/test_alignment_expert_init.py`

**ä»»åŠ¡**:
- [ ] æµ‹è¯•èƒ½å¦æ­£ç¡®åŠ è½½ alignment expert
- [ ] éªŒè¯ä¸‰ä¸ª expert çš„å‚æ•°é‡
  - VLM: ~2B params
  - Action Expert: ~311M params + TTT
  - Alignment Expert: ~311M params
- [ ] æµ‹è¯•å†…å­˜å ç”¨
- [ ] éªŒè¯ gradient checkpointing å…¼å®¹æ€§

**æµ‹è¯•ä»£ç **:
```python
config = Pi0Config(
    use_alignment_expert=True,
    alignment_expert_variant="gemma_300m",
    paligemma_variant="dummy",  # ç”¨ dummy å¿«é€Ÿæµ‹è¯•
    action_expert_variant="dummy",
)
model = Pi0Pytorch(config)
print(f"Total params: {sum(p.numel() for p in model.parameters())}")
```

### 1.2 ä¿®æ”¹ PaliGemmaWithExpertModel.forward() â³
**æ–‡ä»¶**: `src/openpi/models_pytorch/gemma_pytorch.py`

**å½“å‰è¿”å›**: `[prefix_output, suffix_output], past_key_values`

**éœ€è¦æ”¹ä¸º**:
```python
if self.alignment_expert is not None and return_alignment_hidden:
    # è¿è¡Œ alignment expert forward
    alignment_hidden = self.alignment_expert.model.forward(
        inputs_embeds=suffix_embs,  # æˆ–è€…ä» Action Expert è·å–ï¼Ÿ
        attention_mask=...,
        ...
    )
    return [prefix_output, suffix_output], past_key_values, alignment_hidden
else:
    return [prefix_output, suffix_output], past_key_values
```

**å…³é”®é—®é¢˜**:
- Alignment expert çš„è¾“å…¥åº”è¯¥æ˜¯ä»€ä¹ˆï¼Ÿ
  - é€‰é¡¹A: ä¸ Action Expert ç›¸åŒçš„è¾“å…¥ï¼ˆsuffix_embsï¼‰
  - é€‰é¡¹B: Action Expert çš„è¾“å‡ºï¼ˆsuffix_outputï¼‰
  - **å»ºè®®é€‰é¡¹B**: å› ä¸º Action Expert çš„è¾“å‡ºå·²ç»æ˜¯ TTT-conditioned

### 1.3 ä¿®æ”¹ PI0Pytorch.forward() â³
**æ–‡ä»¶**: `src/openpi/models_pytorch/pi0_pytorch.py`

**å½“å‰ä»£ç ** (line 334-391):
```python
def forward(self, observation, actions, noise=None, time=None) -> Tensor:
    # ... é¢„å¤„ç† ...

    # Forward through backbone
    (_, suffix_out), _ = self.paligemma_with_expert.forward(...)

    # Action prediction
    v_t = self.action_out_proj(suffix_out)
    return F.mse_loss(u_t, v_t, reduction="none")
```

**éœ€è¦æ”¹ä¸º**:
```python
def forward(self, observation, actions, noise=None, time=None, obs_next=None) -> Tensor:
    # ... é¢„å¤„ç† ...

    # Forward through backbone
    if self.config.use_alignment_expert and self.training:
        (_, suffix_out), _, alignment_hidden = self.paligemma_with_expert.forward(
            ..., return_alignment_hidden=True
        )
    else:
        (_, suffix_out), _ = self.paligemma_with_expert.forward(...)

    # Action prediction
    v_t = self.action_out_proj(suffix_out)
    action_loss = F.mse_loss(u_t, v_t, reduction="none")

    # Alignment losses (if enabled)
    if self.config.use_alignment_expert and self.training:
        alignment_losses = self._compute_alignment_losses(
            alignment_hidden, actions, obs_next, observation.state
        )
        return action_loss, alignment_losses

    return action_loss
```

---

## Phase 2: æ•°æ®æµæ”¹é€  [P0] (2-3å¤©)

### 2.1 å®ç° _compute_alignment_losses() æ–¹æ³• â³
**æ–‡ä»¶**: `src/openpi/models_pytorch/pi0_pytorch.py`

```python
def _compute_alignment_losses(self, alignment_hidden, actions, obs_next, state):
    """
    è®¡ç®—æ‰€æœ‰ alignment expert çš„æŸå¤±

    Args:
        alignment_hidden: [B, L, hidden_dim] from alignment expert
        actions: [B, action_horizon, action_dim] ground truth actions
        obs_next: Observation (ä¸‹ä¸€å¸§)
        state: [B, state_dim] proprioceptive state

    Returns:
        dict with keys: 'inverse_dynamics', 'dynamics', 'perception'
    """
    losses = {}

    # æå–ç‰¹å¾ (ä½¿ç”¨æœ€åä¸€ä¸ª token)
    feat = alignment_hidden[:, -1, :]  # [B, hidden_dim]

    # Inverse Dynamics Loss
    if obs_next is not None:
        # éœ€è¦è·å– obs_t+1 çš„ hidden states
        # TODO: è¿™éœ€è¦å†æ¬¡ forwardï¼Œæˆ–è€…åœ¨ä¸» forward ä¸­ä¸€æ¬¡æ€§å¤„ç†
        pred_action = self.inverse_dynamics_head(feat)  # [B, action_dim]
        losses['inverse_dynamics'] = F.mse_loss(pred_action, actions[:, 0, :])

    # Dynamics Loss
    pred_obs_next = self.dynamics_head(feat)  # [B, hidden_dim]
    # TODO: éœ€è¦ obs_next çš„ target features

    # Perception Loss
    pred_state = self.perception_head(feat)  # [B, state_dim]
    losses['perception'] = F.mse_loss(pred_state, state)

    return losses
```

**é—®é¢˜**:
- Inverse dynamics éœ€è¦ (obs_t, obs_t+1) çš„è”åˆè¡¨å¾
  - å½“å‰åªæœ‰ obs_t çš„ hidden states
  - éœ€è¦ä¿®æ”¹æ¶æ„æ”¯æŒå¤„ç†ä¸¤å¸§
- Dynamics éœ€è¦ obs_t+1 çš„ target features
  - éœ€è¦é€šè¿‡ vision encoder è·å–

### 2.2 ä¿®æ”¹æ•°æ®åŠ è½½å™¨æ”¯æŒè¿ç»­å¸§ â³
**æ–‡ä»¶**: `src/openpi/training/data/*.py`

**éœ€è¦ä¿®æ”¹**:
- åŠ è½½è¿ç»­çš„ä¸¤å¸§: `(obs_t, obs_t+1)`
- ç¡®ä¿å¯¹åº”çš„ `(action_t, state_t)` ä¹Ÿæ­£ç¡®åŠ è½½

**ä¿®æ”¹ä½ç½®**:
- `LeRobotDataset` æˆ–ç›¸å…³çš„ data transform
- åœ¨ `__getitem__` ä¸­è¿”å›é¢å¤–çš„ `obs_next` å­—æ®µ

### 2.3 åˆ›å»º AlignmentLossComputer â³
**æ–‡ä»¶**: åˆ›å»º `src/openpi/models_pytorch/alignment_loss.py`

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Tuple

class AlignmentLossComputer(nn.Module):
    """è®¡ç®— alignment expert çš„æ‰€æœ‰æŸå¤±"""

    def __init__(
        self,
        lambda_inverse_dynamics: float = 1.0,
        lambda_dynamics: float = 1.0,
        lambda_perception: float = 1.0,
    ):
        super().__init__()
        self.lambda_inv = lambda_inverse_dynamics
        self.lambda_dyn = lambda_dynamics
        self.lambda_per = lambda_perception

    def forward(
        self,
        predictions: Dict[str, torch.Tensor],
        targets: Dict[str, torch.Tensor],
    ) -> Tuple[torch.Tensor, Dict[str, float]]:
        """
        Args:
            predictions: {
                'inverse_dynamics': [B, action_dim],
                'dynamics': [B, hidden_dim],
                'perception': [B, state_dim],
            }
            targets: {
                'action': [B, action_dim],
                'next_obs_features': [B, hidden_dim],
                'state': [B, state_dim],
            }
        """
        loss_dict = {}
        total_loss = 0.0

        # Inverse Dynamics Loss
        if 'inverse_dynamics' in predictions:
            inv_loss = F.mse_loss(
                predictions['inverse_dynamics'],
                targets['action']
            )
            loss_dict['alignment/inverse_dynamics'] = inv_loss.item()
            total_loss += self.lambda_inv * inv_loss

        # Dynamics Loss
        if 'dynamics' in predictions:
            dyn_loss = F.mse_loss(
                predictions['dynamics'],
                targets['next_obs_features']
            )
            loss_dict['alignment/dynamics'] = dyn_loss.item()
            total_loss += self.lambda_dyn * dyn_loss

        # Perception Loss
        if 'perception' in predictions:
            per_loss = F.mse_loss(
                predictions['perception'],
                targets['state']
            )
            loss_dict['alignment/perception'] = per_loss.item()
            total_loss += self.lambda_per * per_loss

        loss_dict['alignment/total'] = total_loss.item()
        return total_loss, loss_dict
```

---

## Phase 3: å¯¹æ¯”å­¦ä¹  [P1] (3-5å¤©)

### 3.1 ç†è§£ W çš„æå–æ–¹å¼ ğŸ”
**å…³é”®é—®é¢˜**: W åœ¨å“ªé‡Œï¼Ÿå¦‚ä½•è®¿é—®ï¼Ÿ

TTT å±‚ä½ç½®: `src/openpi/models_pytorch/transformers_replace/models/gemma/ttt_with_gate.py`

TTT å‚æ•°:
```python
class TTTLinear:
    def __init__(self):
        self.W1 = nn.Parameter(...)  # [num_heads, head_dim, head_dim]
        self.b1 = nn.Parameter(...)  # [num_heads, 1, head_dim]
```

è®¿é—®æ–¹å¼:
```python
# ä» PI0Pytorch è®¿é—®
for layer in self.paligemma_with_expert.gemma_expert.model.layers:
    if hasattr(layer, 'ttt_layer'):
        W1 = layer.ttt_layer.W1  # [num_heads, head_dim, head_dim]
        b1 = layer.ttt_layer.b1  # [num_heads, 1, head_dim]
```

### 3.2 è®¾è®¡å¯¹æ¯”å­¦ä¹ ç­–ç•¥ ğŸ¯

#### æ­£æ ·æœ¬æ„é€ 
**ç›®æ ‡**: è®©ç›¸åŒ embodiment çš„ W ä¿æŒæ¥è¿‘

**æ–¹æ³•**: Observation appearance augmentation
- é¢œè‰²æŠ–åŠ¨ (color jitter)
- äº®åº¦ã€å¯¹æ¯”åº¦å˜åŒ–
- é«˜æ–¯å™ªå£°
- **ä¸æ”¹å˜ layout**: ä¸ç¿»è½¬ã€ä¸è£å‰ªã€ä¸æ—‹è½¬

**ä¸ºä»€ä¹ˆ**: å¤–è§‚å˜åŒ–ä¸æ”¹å˜ embodiment contextï¼ˆåŠ¨ä½œç©ºé—´ã€åŠ¨åŠ›å­¦ã€åæ ‡ç³»éƒ½æ²¡å˜ï¼‰

#### è´Ÿæ ·æœ¬æ„é€ 
**ç›®æ ‡**: è®©ä¸åŒ embodiment çš„ W è·ç¦»æ‹‰è¿œ

**æ–¹æ³•**: Embodiment configuration perturbation
1. **Action space è½¬æ¢**:
   - Cartesian space (x,y,z,roll,pitch,yaw) â†” Joint space (Î¸1,...,Î¸7)
   - Delta actions â†” Absolute actions

2. **åæ ‡ç³»è½¬æ¢**:
   - Base frame â†” World frame
   - Camera frame â†” Robot frame

3. **DOF å˜åŒ–**:
   - 7-DOF (Franka) â†” 6-DOF (UR5)
   - æ·»åŠ /ç§»é™¤æŸäº›ç»´åº¦

**ä¸ºä»€ä¹ˆ**: è¿™äº›å˜åŒ–ç›´æ¥å½±å“ action prediction çš„æ–¹å¼ï¼Œåº”è¯¥è¢«ç¼–ç åœ¨ W ä¸­

### 3.3 å®ç° EmbodimentContrastiveLoss â³
**æ–‡ä»¶**: åˆ›å»º `src/openpi/models_pytorch/contrastive_loss.py`

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class EmbodimentContrastiveLoss(nn.Module):
    """
    å¯¹æ¯”å­¦ä¹  lossï¼Œç”¨äºåˆ†ç¦» embodiment-agnostic å’Œ embodiment-relevant è¡¨å¾

    æ ¸å¿ƒæ€æƒ³:
    - W (TTT å‚æ•°) åº”è¯¥ç¼–ç  embodiment context
    - ç›¸åŒ embodiment (appearance augmentation) çš„ W åº”è¯¥æ¥è¿‘
    - ä¸åŒ embodiment (configuration perturbation) çš„ W åº”è¯¥è¿œç¦»
    """

    def __init__(self, temperature: float = 0.07):
        super().__init__()
        self.temperature = temperature

    def extract_W_embedding(self, model) -> torch.Tensor:
        """
        ä» TTT å±‚æå– W å¹¶ flatten æˆ embedding

        Returns:
            W_emb: [B, W_dim] å¦‚æœæ”¯æŒ batch-level W
                   æˆ– [W_dim] å¦‚æœæ˜¯ global W
        """
        W_list = []
        for layer in model.paligemma_with_expert.gemma_expert.model.layers:
            if hasattr(layer, 'ttt_layer'):
                W1 = layer.ttt_layer.W1  # [num_heads, head_dim, head_dim]
                b1 = layer.ttt_layer.b1  # [num_heads, 1, head_dim]
                # Flatten and concatenate
                W_list.append(W1.flatten())
                W_list.append(b1.flatten())

        W_emb = torch.cat(W_list, dim=0)  # [W_dim]
        return W_emb

    def forward(
        self,
        model_anchor,
        model_positive,
        model_negative,
    ) -> torch.Tensor:
        """
        InfoNCE loss for embodiment context

        Args:
            model_anchor: åŸå§‹æ¨¡å‹
            model_positive: ç»è¿‡ appearance augmentation çš„æ¨¡å‹
            model_negative: ç»è¿‡ configuration perturbation çš„æ¨¡å‹
        """
        W_anchor = self.extract_W_embedding(model_anchor)
        W_pos = self.extract_W_embedding(model_positive)
        W_neg = self.extract_W_embedding(model_negative)

        # Normalize embeddings
        W_anchor = F.normalize(W_anchor, dim=-1)
        W_pos = F.normalize(W_pos, dim=-1)
        W_neg = F.normalize(W_neg, dim=-1)

        # Compute similarities
        pos_sim = torch.sum(W_anchor * W_pos, dim=-1) / self.temperature
        neg_sim = torch.sum(W_anchor * W_neg, dim=-1) / self.temperature

        # InfoNCE loss
        logits = torch.cat([pos_sim.unsqueeze(0), neg_sim.unsqueeze(0)], dim=0)
        labels = torch.zeros(1, dtype=torch.long, device=logits.device)

        loss = F.cross_entropy(logits.unsqueeze(0), labels)

        return loss
```

**é—®é¢˜**:
- å½“å‰ TTT å®ç°æ˜¯å¦æ”¯æŒ batch-level çš„ä¸åŒ Wï¼Ÿ
  - æŸ¥çœ‹ `ttt_with_gate.py` çš„å®ç°
  - å¯èƒ½éœ€è¦ä¿®æ”¹ä¸ºæ”¯æŒ per-sample W

### 3.4 å®ç°æ•°æ®å¢å¼º Pipeline â³
**æ–‡ä»¶**: åˆ›å»º `src/openpi/training/data/embodiment_augmentation.py`

```python
import torch
import torchvision.transforms as T

class AppearanceAugmentation:
    """å¤–è§‚å¢å¼ºï¼ˆæ­£æ ·æœ¬ï¼‰"""
    def __init__(self):
        self.transform = T.Compose([
            T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
            T.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),
        ])

    def __call__(self, image):
        return self.transform(image)

class EmbodimentConfigurationAugmentation:
    """é…ç½®å¢å¼ºï¼ˆè´Ÿæ ·æœ¬ï¼‰"""
    def __init__(self):
        pass

    def transform_action_space(self, action, state, mode='cartesian_to_joint'):
        """
        è½¬æ¢ action space

        Args:
            action: [action_horizon, action_dim]
            state: [state_dim] current robot state
            mode: 'cartesian_to_joint' or 'joint_to_cartesian'
        """
        if mode == 'cartesian_to_joint':
            # ä½¿ç”¨é€†è¿åŠ¨å­¦è½¬æ¢
            # éœ€è¦æœºå™¨äººçš„è¿åŠ¨å­¦æ¨¡å‹
            pass
        elif mode == 'joint_to_cartesian':
            # ä½¿ç”¨æ­£è¿åŠ¨å­¦è½¬æ¢
            pass
        return transformed_action

    def transform_frame(self, action, state, from_frame='base', to_frame='world'):
        """
        è½¬æ¢åæ ‡ç³»

        éœ€è¦çŸ¥é“ä¸¤ä¸ªåæ ‡ç³»ä¹‹é—´çš„å˜æ¢çŸ©é˜µ
        """
        # Apply transformation matrix
        pass
        return transformed_action
```

---

## Phase 4: è®­ç»ƒé…ç½® [P0] (1-2å¤©)

### 4.1 æ·»åŠ  AlignmentConfig â³
**æ–‡ä»¶**: `src/openpi/training/config.py`

```python
@dataclasses.dataclass
class AlignmentConfig:
    """Alignment expert è®­ç»ƒé…ç½®"""

    # Loss weights
    lambda_inverse_dynamics: float = 1.0
    lambda_dynamics: float = 1.0
    lambda_perception: float = 1.0
    lambda_contrastive: float = 0.1

    # Contrastive learning
    use_contrastive: bool = False
    contrastive_temperature: float = 0.07
    num_negative_samples: int = 4

    # Data augmentation
    use_appearance_aug: bool = True
    use_embodiment_aug: bool = False  # éœ€è¦è¿åŠ¨å­¦æ¨¡å‹æ”¯æŒ

# åœ¨ TrainConfig ä¸­æ·»åŠ 
@dataclasses.dataclass
class TrainConfig:
    # ... ç°æœ‰å­—æ®µ ...

    # Alignment expert config
    alignment: AlignmentConfig = dataclasses.field(default_factory=AlignmentConfig)
```

### 4.2 åˆ›å»ºè®­ç»ƒé…ç½®ç¤ºä¾‹ â³
**æ–‡ä»¶**: `src/openpi/training/config.py`

æ·»åŠ æ–°çš„ TrainConfig:
```python
TrainConfig(
    name="pi05_alignment_debug",
    model=pi0_config.Pi0Config(
        pi05=True,
        use_alignment_expert=True,
        alignment_expert_variant="gemma_300m",
        use_ttt=True,
        ttt_layer_positions="all",
        paligemma_variant="dummy",
        action_expert_variant="dummy",
    ),
    data=FakeDataConfig(),  # å…ˆç”¨ fake data æµ‹è¯•
    alignment=AlignmentConfig(
        lambda_inverse_dynamics=1.0,
        lambda_dynamics=1.0,
        lambda_perception=1.0,
        use_contrastive=False,  # å…ˆä¸ç”¨å¯¹æ¯”å­¦ä¹ 
    ),
    batch_size=2,
    num_train_steps=100,
    overwrite=True,
    exp_name="alignment_debug",
    wandb_enabled=False,
)
```

### 4.3 ä¿®æ”¹è®­ç»ƒå¾ªç¯ â³
**æ–‡ä»¶**: `src/openpi/training/train.py`

**å½“å‰ä»£ç ** (éœ€è¦æ‰¾åˆ°å…·ä½“ä½ç½®):
```python
# è®­ç»ƒå¾ªç¯
loss = model.forward(batch)
loss.backward()
optimizer.step()
```

**éœ€è¦æ”¹ä¸º**:
```python
if config.model.use_alignment_expert:
    # Forward with alignment
    action_loss, alignment_losses = model.forward(
        observation=batch['observation'],
        actions=batch['actions'],
        obs_next=batch['obs_next'],  # æ–°å¢
    )

    # è®¡ç®—æ€» loss
    total_loss = action_loss.mean()

    # æ·»åŠ  alignment losses
    if alignment_losses is not None:
        alignment_loss_computer = AlignmentLossComputer(
            lambda_inverse_dynamics=config.alignment.lambda_inverse_dynamics,
            lambda_dynamics=config.alignment.lambda_dynamics,
            lambda_perception=config.alignment.lambda_perception,
        )
        alignment_total, alignment_metrics = alignment_loss_computer(
            predictions=alignment_losses['predictions'],
            targets=alignment_losses['targets'],
        )
        total_loss = total_loss + alignment_total

        # Log alignment metrics
        for key, value in alignment_metrics.items():
            wandb.log({key: value})
else:
    # åŸæœ‰æµç¨‹
    total_loss = model.forward(batch)

total_loss.backward()
optimizer.step()
```

---

## Phase 5: è‡ªå¯¹é½è®­ç»ƒæµç¨‹ [P0] (3-4å¤©)

### 5.1 ç†è§£ä¸¤é˜¶æ®µè®­ç»ƒ ğŸ¯

#### Stage 1: é¢„è®­ç»ƒï¼ˆç°æœ‰æµç¨‹ï¼‰
**ç›®æ ‡**: åœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šå­¦ä¹  embodiment-agnostic è¡¨å¾

**è®­ç»ƒå†…å®¹**:
- VLM: è§†è§‰-è¯­è¨€å¯¹é½
- Action Expert + TTT: å­¦ä¼šå¦‚ä½•åˆ©ç”¨ TTT è¿›è¡Œ adaptation
- Alignment Experts: å­¦ä¼šé¢„æµ‹ alignment ä¿¡å·

**æ•°æ®**: å¤§è§„æ¨¡å¤šæœºå™¨äººæ•°æ®é›†

**ä¼˜åŒ–**: æ‰€æœ‰å‚æ•°

#### Stage 2: è‡ªå¯¹é½ï¼ˆæ–°æµç¨‹ï¼‰
**ç›®æ ‡**: åœ¨ç›®æ ‡ç¯å¢ƒä½¿ç”¨ play data å¿«é€Ÿé€‚åº”

**è®­ç»ƒå†…å®¹**:
- **åªä¼˜åŒ– W (TTT å‚æ•°)**
- å†»ç»“æ‰€æœ‰å…¶ä»–å‚æ•°
- ä½¿ç”¨ alignment expert çš„è‡ªç›‘ç£ä¿¡å·

**æ•°æ®**: ç›®æ ‡ç¯å¢ƒçš„ play dataï¼ˆæ— æ ‡æ³¨ï¼ï¼‰

**ä¼˜åŒ–**: åªæœ‰ TTT å±‚çš„ W1, b1

### 5.2 å®ç° Self-Alignment è„šæœ¬ â³
**æ–‡ä»¶**: åˆ›å»º `src/openpi/training/self_alignment.py`

```python
"""
Self-Alignment Script

ä½¿ç”¨ play data åœ¨æ–°ç¯å¢ƒä¸­å¯¹é½ embodiment context (W)
"""

import torch
from torch import nn
from torch.optim import Adam
from openpi.models_pytorch.pi0_pytorch import Pi0Pytorch
from openpi.models_pytorch.alignment_loss import AlignmentLossComputer

class SelfAlignmentTrainer:
    """è‡ªå¯¹é½è®­ç»ƒå™¨"""

    def __init__(
        self,
        model: Pi0Pytorch,
        learning_rate: float = 1e-4,
        num_steps: int = 1000,
    ):
        self.model = model
        self.num_steps = num_steps

        # å†»ç»“æ‰€æœ‰å‚æ•°
        for param in model.parameters():
            param.requires_grad = False

        # åªè§£å†» TTT å‚æ•°
        ttt_params = []
        for layer in model.paligemma_with_expert.gemma_expert.model.layers:
            if hasattr(layer, 'ttt_layer'):
                for param in layer.ttt_layer.parameters():
                    param.requires_grad = True
                    ttt_params.append(param)

        print(f"Optimizing {len(ttt_params)} TTT parameters")

        # åªä¼˜åŒ– W
        self.optimizer = Adam(ttt_params, lr=learning_rate)

        # Alignment loss computer
        self.loss_computer = AlignmentLossComputer(
            lambda_inverse_dynamics=1.0,
            lambda_dynamics=1.0,
            lambda_perception=1.0,
        )

    def alignment_step(self, batch):
        """
        å•æ­¥è‡ªå¯¹é½

        Args:
            batch: play data (æ— æ ‡æ³¨)
                - observation: å½“å‰è§‚æµ‹
                - obs_next: ä¸‹ä¸€å¸§è§‚æµ‹
                - action: æ‰§è¡Œçš„åŠ¨ä½œï¼ˆä» play data ä¸­è®°å½•ï¼‰
                - state: proprioceptive state
        """
        self.model.train()

        # Forward (åªä½¿ç”¨ alignment expertsï¼Œä¸éœ€è¦ action prediction)
        _, alignment_outputs = self.model.forward_alignment_only(
            observation=batch['observation'],
            obs_next=batch['obs_next'],
        )

        # è®¡ç®— alignment losses
        alignment_loss, metrics = self.loss_computer(
            predictions=alignment_outputs['predictions'],
            targets={
                'action': batch['action'],
                'next_obs_features': alignment_outputs['targets']['next_obs_features'],
                'state': batch['state'],
            }
        )

        # Backward (åªæ›´æ–° W)
        self.optimizer.zero_grad()
        alignment_loss.backward()
        self.optimizer.step()

        return metrics

    def train(self, play_dataloader):
        """
        å®Œæ•´çš„è‡ªå¯¹é½è®­ç»ƒæµç¨‹

        Args:
            play_dataloader: ç›®æ ‡ç¯å¢ƒçš„ play data
        """
        print("Starting self-alignment training...")
        print(f"Total steps: {self.num_steps}")

        step = 0
        while step < self.num_steps:
            for batch in play_dataloader:
                metrics = self.alignment_step(batch)

                if step % 10 == 0:
                    print(f"Step {step}: {metrics}")

                step += 1
                if step >= self.num_steps:
                    break

        print("Self-alignment training completed!")

    def save_adapted_model(self, path):
        """ä¿å­˜é€‚åº”åçš„æ¨¡å‹ï¼ˆåªéœ€è¦ä¿å­˜ Wï¼‰"""
        ttt_state = {}
        for name, layer in enumerate(self.model.paligemma_with_expert.gemma_expert.model.layers):
            if hasattr(layer, 'ttt_layer'):
                ttt_state[f'layer_{name}'] = {
                    'W1': layer.ttt_layer.W1.data.clone(),
                    'b1': layer.ttt_layer.b1.data.clone(),
                }
        torch.save(ttt_state, path)
        print(f"Saved adapted TTT parameters to {path}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    config = Pi0Config(
        use_alignment_expert=True,
        use_ttt=True,
        # ...
    )
    model = Pi0Pytorch(config)
    model.load_state_dict(torch.load("pretrained_model.pth"))

    # 2. å‡†å¤‡ play data
    play_dataset = PlayDataset(data_dir="./play_data")
    play_dataloader = DataLoader(play_dataset, batch_size=8)

    # 3. è‡ªå¯¹é½è®­ç»ƒ
    trainer = SelfAlignmentTrainer(model, learning_rate=1e-4, num_steps=1000)
    trainer.train(play_dataloader)

    # 4. ä¿å­˜é€‚åº”åçš„ W
    trainer.save_adapted_model("adapted_ttt_params.pth")
```

### 5.3 æ·»åŠ  forward_alignment_only() æ–¹æ³• â³
**æ–‡ä»¶**: `src/openpi/models_pytorch/pi0_pytorch.py`

```python
def forward_alignment_only(self, observation, obs_next):
    """
    åªä½¿ç”¨ alignment experts è¿›è¡Œå‰å‘ä¼ æ’­ï¼ˆç”¨äºè‡ªå¯¹é½ï¼‰

    ä¸éœ€è¦ action labelsï¼Œåªä½¿ç”¨è‡ªç›‘ç£ä¿¡å·

    Args:
        observation: å½“å‰è§‚æµ‹
        obs_next: ä¸‹ä¸€å¸§è§‚æµ‹

    Returns:
        alignment_outputs: {
            'predictions': {...},
            'targets': {...},
        }
    """
    # 1. è·å– obs_t çš„ hidden states
    # ... (ç±»ä¼¼ forward çš„å¤„ç†)

    # 2. è·å– obs_t+1 çš„ hidden states
    # ... (éœ€è¦å†æ¬¡ forward)

    # 3. ä½¿ç”¨ alignment expert
    alignment_hidden_t = ...
    alignment_hidden_t1 = ...

    # 4. é¢„æµ‹
    predictions = {
        'inverse_dynamics': self.inverse_dynamics_head(...),
        'dynamics': self.dynamics_head(...),
        'perception': self.perception_head(...),
    }

    # 5. æ„é€  targets (ä» play data ä¸­è·å–)
    targets = {
        # è¿™äº›ä¼šåœ¨å¤–éƒ¨æä¾›
    }

    return {'predictions': predictions, 'targets': targets}
```

---

## Phase 6: å®éªŒéªŒè¯ [P1] (3-5å¤©)

### 6.1 æ¶ˆèå®éªŒè®¾è®¡ ğŸ“Š

#### å®éªŒé…ç½®
| å®éªŒåç§° | Inverse Dyn | Dynamics | Perception | Contrastive | è¯´æ˜ |
|---------|------------|----------|-----------|-------------|------|
| Baseline | âŒ | âŒ | âŒ | âŒ | ä¸ä½¿ç”¨ alignment |
| +InvDyn | âœ… | âŒ | âŒ | âŒ | åªç”¨é€†åŠ¨åŠ›å­¦ |
| +Dyn | âŒ | âœ… | âŒ | âŒ | åªç”¨åŠ¨åŠ›å­¦ |
| +Percept | âŒ | âŒ | âœ… | âŒ | åªç”¨æ„ŸçŸ¥å¯¹é½ |
| +All | âœ… | âœ… | âœ… | âŒ | æ‰€æœ‰ alignment |
| +Contrast | âœ… | âœ… | âœ… | âœ… | å®Œæ•´æ–¹æ³• |

#### è¯„ä¼°æŒ‡æ ‡
1. **é›¶æ ·æœ¬æˆåŠŸç‡**: ä¸ä½¿ç”¨ç›®æ ‡ç¯å¢ƒçš„ä»»ä½•æ¼”ç¤ºæ•°æ®
2. **å°‘æ ·æœ¬æ€§èƒ½**: ä½¿ç”¨ N æ¡ play trajectories åçš„æˆåŠŸç‡
3. **é€‚åº”é€Ÿåº¦**: è¾¾åˆ° X% æˆåŠŸç‡éœ€è¦çš„ play data é‡
4. **è®¡ç®—æ•ˆç‡**: è‡ªå¯¹é½è®­ç»ƒçš„æ—¶é—´å’Œå†…å­˜

### 6.2 è¿ç§»åœºæ™¯æµ‹è¯• ğŸ¯

#### åœºæ™¯1: ç›¸æœºè§†è§’å˜åŒ–
**è®¾ç½®**:
- è®­ç»ƒ: Franka robot, å›ºå®šç›¸æœºè§†è§’ A
- æµ‹è¯•: Franka robot, æ–°ç›¸æœºè§†è§’ B
- Play data: æœºå™¨äººåœ¨è§†è§’ B ä¸‹éšæœºè¿åŠ¨ 100 æ¡è½¨è¿¹

**æœŸæœ›**:
- W åº”è¯¥èƒ½æ•è·ç›¸æœºå¤–å‚çš„å˜åŒ–
- Perception expert å¸®åŠ©ç†è§£æ–°è§†è§’ä¸‹çš„è§‚æµ‹

#### åœºæ™¯2: Action Space å˜åŒ–
**è®¾ç½®**:
- è®­ç»ƒ: Cartesian space control (x,y,z,roll,pitch,yaw)
- æµ‹è¯•: Joint space control (Î¸1,...,Î¸7)
- Play data: å…³èŠ‚ç©ºé—´çš„éšæœºè¿åŠ¨

**æœŸæœ›**:
- Inverse dynamics expert å­¦ä¹ æ–°çš„ action space æ˜ å°„
- W ç¼–ç  action space çš„ç‰¹æ€§

#### åœºæ™¯3: è·¨æœºå™¨äººå¹³å°
**è®¾ç½®**:
- è®­ç»ƒ: Franka Panda (7-DOF)
- æµ‹è¯•: UR5 (6-DOF)
- Play data: UR5 çš„éšæœºè¿åŠ¨

**æœŸæœ›**:
- è¿™æ˜¯æœ€éš¾çš„åœºæ™¯
- éœ€è¦æ‰€æœ‰ alignment experts é…åˆ
- å¯èƒ½éœ€è¦æ›´å¤š play data

### 6.3 å®éªŒè„šæœ¬ â³
**æ–‡ä»¶**: åˆ›å»º `experiments/alignment_ablation.py`

```python
"""
Alignment Expert æ¶ˆèå®éªŒ
"""

import torch
from openpi.models_pytorch.pi0_pytorch import Pi0Pytorch
from openpi.training.self_alignment import SelfAlignmentTrainer

def run_ablation_experiment(
    alignment_config,
    play_data_path,
    test_tasks,
):
    """
    è¿è¡Œå•ä¸ªæ¶ˆèå®éªŒ

    Args:
        alignment_config: dict with expert switches
            {
                'use_inverse_dynamics': True/False,
                'use_dynamics': True/False,
                'use_perception': True/False,
                'use_contrastive': True/False,
            }
        play_data_path: path to play data
        test_tasks: list of test tasks
    """
    # 1. åˆ›å»ºæ¨¡å‹
    config = Pi0Config(
        use_alignment_expert=True,
        # ... set based on alignment_config
    )
    model = Pi0Pytorch(config)
    model.load_pretrained("pretrained_checkpoint.pth")

    # 2. è‡ªå¯¹é½è®­ç»ƒ
    trainer = SelfAlignmentTrainer(model)
    trainer.train(play_dataloader)

    # 3. é›¶æ ·æœ¬è¯„ä¼°
    zero_shot_results = evaluate_tasks(model, test_tasks)

    # 4. å°‘æ ·æœ¬è¯„ä¼°
    few_shot_results = {}
    for num_demos in [1, 5, 10, 20]:
        # Fine-tune with demonstrations
        finetuned_model = finetune_with_demos(model, num_demos)
        few_shot_results[num_demos] = evaluate_tasks(finetuned_model, test_tasks)

    return {
        'zero_shot': zero_shot_results,
        'few_shot': few_shot_results,
    }

if __name__ == "__main__":
    # è¿è¡Œæ‰€æœ‰æ¶ˆèå®éªŒ
    ablations = [
        {'name': 'baseline', 'use_inverse_dynamics': False, ...},
        {'name': '+inv_dyn', 'use_inverse_dynamics': True, ...},
        # ... å…¶ä»–é…ç½®
    ]

    results = {}
    for ablation in ablations:
        print(f"Running ablation: {ablation['name']}")
        results[ablation['name']] = run_ablation_experiment(
            ablation,
            play_data_path="./play_data",
            test_tasks=["pick_cube", "place_cup", ...],
        )

    # ä¿å­˜å’Œå¯è§†åŒ–ç»“æœ
    save_results(results, "ablation_results.json")
    plot_results(results, "ablation_plots.pdf")
```

---

## Phase 7: æ–‡æ¡£å’Œä¼˜åŒ– [P2] (æŒç»­)

### 7.1 åˆ›å»ºè¯¦ç»†æ–‡æ¡£ ğŸ“

#### æ–‡æ¡£åˆ—è¡¨
- [x] `self_alignment_implementation_plan.md` (æœ¬æ–‡æ¡£)
- [ ] `self_alignment_architecture.md` - è¯¦ç»†æ¶æ„è®¾è®¡
- [ ] `alignment_experts_design.md` - æ¯ä¸ª expert çš„è®¾è®¡ç»†èŠ‚
- [ ] `contrastive_learning_strategy.md` - å¯¹æ¯”å­¦ä¹ ç­–ç•¥
- [ ] `self_alignment_tutorial.md` - ä½¿ç”¨æ•™ç¨‹
- [ ] `experiment_results.md` - å®éªŒç»“æœå’Œåˆ†æ

### 7.2 æ€§èƒ½ä¼˜åŒ– âš¡

#### å†…å­˜ä¼˜åŒ–
- [ ] Alignment expert ä½¿ç”¨ gradient checkpointing
- [ ] è€ƒè™‘æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16/BF16ï¼‰
- [ ] å‡å°‘ alignment expert å±‚æ•°ï¼ˆä» 18 å±‚å‡åˆ° 4-6 å±‚ï¼Ÿï¼‰

#### è®¡ç®—ä¼˜åŒ–
- [ ] æ˜¯å¦å¯ä»¥å¹¶è¡Œå¤„ç† obs_t å’Œ obs_t+1ï¼Ÿ
- [ ] ç¼“å­˜é‡å¤è®¡ç®—çš„ features
- [ ] ä½¿ç”¨ torch.compile() åŠ é€Ÿ

#### è®­ç»ƒé€Ÿåº¦ä¼˜åŒ–
- [ ] ä½¿ç”¨æ›´å¤§çš„ batch sizeï¼ˆé€šè¿‡ gradient accumulationï¼‰
- [ ] å¼‚æ­¥æ•°æ®åŠ è½½
- [ ] å¤š GPU è®­ç»ƒæ”¯æŒ

### 7.3 ä»£ç è´¨é‡ âœ¨
- [ ] æ·»åŠ å•å…ƒæµ‹è¯•
- [ ] æ·»åŠ ç±»å‹æ³¨è§£
- [ ] ä»£ç  review å’Œé‡æ„
- [ ] æ·»åŠ  docstrings

---

## å…³é”®å†³ç­–ç‚¹ ğŸ”‘

éœ€è¦åšå‡ºçš„é‡è¦å†³å®šï¼š

### 1. W çš„å½¢å¼å’Œç®¡ç†
**é—®é¢˜**: W åº”è¯¥å¦‚ä½•ç»„ç»‡ï¼Ÿ

**é€‰é¡¹**:
- **A**: å…¨å±€å•ä¸€çš„ Wï¼ˆæ‰€æœ‰ samples å…±äº«ï¼‰
  - ç®€å•ï¼Œä½†æ— æ³•å¤„ç† batch å†…çš„ embodiment å·®å¼‚

- **B**: Per-sample Wï¼ˆæ¯ä¸ª sample ç‹¬ç«‹çš„ Wï¼‰
  - çµæ´»ï¼Œä½†éœ€è¦ä¿®æ”¹ TTT å®ç°
  - å†…å­˜å ç”¨æ›´å¤§

- **C**: Per-embodiment Wï¼ˆæ¯ä¸ª embodiment ç±»å‹ä¸€ä¸ª Wï¼‰
  - æŠ˜ä¸­æ–¹æ¡ˆï¼Œéœ€è¦ embodiment ID

**å»ºè®®**: å…ˆç”¨ Aï¼ˆå…¨å±€ Wï¼‰ï¼ŒéªŒè¯æ¦‚å¿µåå†è€ƒè™‘ B

### 2. Alignment Expert çš„æ¶æ„æ·±åº¦
**é—®é¢˜**: Alignment expert éœ€è¦å¤šå°‘å±‚ï¼Ÿ

**å½“å‰**: 18 å±‚ï¼ˆgemma_300mï¼‰

**å¯èƒ½çš„ä¼˜åŒ–**:
- å‡å°‘åˆ° 4-6 å±‚
- åªéœ€è¦è½»é‡çº§çš„ predictionï¼Œä¸éœ€è¦å®Œæ•´çš„è¯­è¨€ç†è§£

**å»ºè®®**:
1. å…ˆç”¨ 18 å±‚éªŒè¯åŠŸèƒ½
2. åšæ¶ˆèå®éªŒæµ‹è¯• 4/6/8/18 å±‚çš„æ€§èƒ½å·®å¼‚
3. åˆ›å»ºæ–°çš„ variant: `gemma_300m_4layer`

### 3. Inverse Dynamics çš„è¾“å…¥è¡¨å¾
**é—®é¢˜**: å¦‚ä½•è®© inverse dynamics expert çœ‹åˆ°ä¸¤å¸§ï¼Ÿ

**é€‰é¡¹**:
- **A**: Concatenate hidden states from obs_t and obs_t+1
  - éœ€è¦ä¸¤æ¬¡ forward pass
  - è®¡ç®—å¼€é”€å¤§

- **B**: ç”¨ temporal attention å¤„ç†åºåˆ— [obs_t, obs_t+1]
  - æ›´ä¼˜é›…ï¼Œä½†éœ€è¦ä¿®æ”¹æ¶æ„

- **C**: ç®€åŒ–ä¸ºå•å¸§é¢„æµ‹ï¼ˆæ”¾å¼ƒ inverse dynamicsï¼‰
  - æŸå¤±é‡è¦çš„è‡ªç›‘ç£ä¿¡å·

**å»ºè®®**: å…ˆç”¨ Aï¼ŒéªŒè¯åè€ƒè™‘ B

### 4. å¯¹æ¯”å­¦ä¹ çš„ä¼˜å…ˆçº§
**é—®é¢˜**: å¯¹æ¯”å­¦ä¹ ä½•æ—¶å®ç°ï¼Ÿ

**åˆ†æ**:
- å¯¹æ¯”å­¦ä¹ ç†è®ºä¸Šå¾ˆé‡è¦ï¼ˆåˆ†ç¦»ä¸¤ç§è¡¨å¾ï¼‰
- ä½†å®ç°å¤æ‚åº¦é«˜
- å¯èƒ½ä¸æ˜¯ MVP çš„å¿…éœ€é¡¹

**å»ºè®®**:
1. Phase 1-2: å…ˆä¸åšå¯¹æ¯”å­¦ä¹ ï¼Œåªç”¨ alignment experts
2. éªŒè¯ alignment experts æœ¬èº«æ˜¯å¦æœ‰æ•ˆ
3. Phase 3: å†æ·»åŠ å¯¹æ¯”å­¦ä¹ ï¼Œçœ‹æ˜¯å¦æœ‰æå‡

### 5. æ•°æ®å¢å¼ºçš„å¤æ‚åº¦
**é—®é¢˜**: Embodiment configuration augmentation éœ€è¦è¿åŠ¨å­¦æ¨¡å‹å—ï¼Ÿ

**åˆ†æ**:
- ç†æƒ³æƒ…å†µï¼šéœ€è¦å‡†ç¡®çš„è¿åŠ¨å­¦æ¨¡å‹åšåæ ‡è½¬æ¢
- å®é™…æƒ…å†µï¼šå¯èƒ½æ²¡æœ‰æ‰€æœ‰æœºå™¨äººçš„è¿åŠ¨å­¦æ¨¡å‹

**æ›¿ä»£æ–¹æ¡ˆ**:
- ç®€å•çš„ linear transformation
- ä½¿ç”¨å†å²æ•°æ®åšç»éªŒæ€§çš„æ˜ å°„
- å…ˆä¸åš negative samplesï¼Œåªç”¨ positive samples

**å»ºè®®**: å…ˆç”¨ç®€å•çš„ linear transformation éªŒè¯æ¦‚å¿µ

---

## æ—¶é—´çº¿å’Œé‡Œç¨‹ç¢‘ ğŸ“…

### Week 1: åŸºç¡€æ¶æ„ (Phase 1-2)
- [ ] Day 1-2: æµ‹è¯•åˆå§‹åŒ–ï¼Œä¿®æ”¹ forward æµç¨‹
- [ ] Day 3-4: æ•°æ®åŠ è½½å™¨æ”¹é€ ï¼Œæ”¯æŒè¿ç»­å¸§
- [ ] Day 5: å®ç° AlignmentLossComputer
- [ ] Day 6-7: ç«¯åˆ°ç«¯æµ‹è¯•ï¼Œç¡®ä¿èƒ½æ­£ç¡®è®­ç»ƒ

**é‡Œç¨‹ç¢‘**: èƒ½å¤Ÿè®­ç»ƒä¸€ä¸ªå¸¦ alignment experts çš„æ¨¡å‹

### Week 2: è®­ç»ƒé…ç½®å’Œè‡ªå¯¹é½ (Phase 4-5)
- [ ] Day 8-9: æ·»åŠ è®­ç»ƒé…ç½®ï¼Œä¿®æ”¹è®­ç»ƒå¾ªç¯
- [ ] Day 10-11: å®ç° SelfAlignmentTrainer
- [ ] Day 12-13: æµ‹è¯•è‡ªå¯¹é½æµç¨‹
- [ ] Day 14: Debug å’Œä¼˜åŒ–

**é‡Œç¨‹ç¢‘**: èƒ½å¤Ÿä½¿ç”¨ play data è¿›è¡Œè‡ªå¯¹é½

### Week 3: å¯¹æ¯”å­¦ä¹ å’Œå®éªŒ (Phase 3, 6)
- [ ] Day 15-17: å®ç°å¯¹æ¯”å­¦ä¹ ï¼ˆå¦‚æœéœ€è¦ï¼‰
- [ ] Day 18-19: è¿è¡Œæ¶ˆèå®éªŒ
- [ ] Day 20-21: é›¶æ ·æœ¬è¿ç§»æµ‹è¯•

**é‡Œç¨‹ç¢‘**: æœ‰åˆæ­¥çš„å®éªŒç»“æœ

### Week 4: ä¼˜åŒ–å’Œæ–‡æ¡£ (Phase 7)
- [ ] Day 22-24: æ€§èƒ½ä¼˜åŒ–å’Œ bug ä¿®å¤
- [ ] Day 25-27: å®Œå–„æ–‡æ¡£
- [ ] Day 28: æœ€ç»ˆ review

**é‡Œç¨‹ç¢‘**: å¯å‘å¸ƒçš„å®Œæ•´å®ç°

---

## é£é™©å’ŒæŒ‘æˆ˜ âš ï¸

### æŠ€æœ¯é£é™©

1. **TTT å±‚å¯èƒ½ä¸æ”¯æŒ per-sample W**
   - å½“å‰å®ç°å¯èƒ½æ˜¯å…¨å±€ W
   - éœ€è¦ä¿®æ”¹åº•å±‚å®ç°
   - **ç¼“è§£**: å…ˆç”¨å…¨å±€ W éªŒè¯æ¦‚å¿µ

2. **Alignment expert æ•ˆæœå¯èƒ½ä¸æ˜æ˜¾**
   - è‡ªç›‘ç£ä¿¡å·å¯èƒ½ä¸å¤Ÿå¼º
   - W å¯èƒ½æ— æ³•æœ‰æ•ˆç¼–ç  embodiment
   - **ç¼“è§£**: åšå……åˆ†çš„æ¶ˆèå®éªŒ

3. **å†…å­˜å’Œè®¡ç®—å¼€é”€**
   - å¤šäº†ä¸€ä¸ª 300M å‚æ•°çš„ expert
   - éœ€è¦å¤„ç†è¿ç»­ä¸¤å¸§
   - **ç¼“è§£**: ä½¿ç”¨ gradient checkpointingï¼Œå‡å°‘å±‚æ•°

4. **æ•°æ®åŠ è½½å™¨æ”¹é€ å¤æ‚**
   - ç°æœ‰ dataloader å¯èƒ½ä¸æ”¯æŒè¿ç»­å¸§
   - éœ€è¦ç¡®ä¿æ—¶åºå¯¹é½
   - **ç¼“è§£**: é€æ­¥ä¿®æ”¹ï¼Œå……åˆ†æµ‹è¯•

### å®éªŒé£é™©

1. **é›¶æ ·æœ¬æ€§èƒ½å¯èƒ½å¾ˆå·®**
   - Domain gap å¯èƒ½å¤ªå¤§
   - Play data å¯èƒ½ä¸å¤Ÿ
   - **ç¼“è§£**: é™ä½æœŸæœ›ï¼Œå…ˆæµ‹è¯•ç®€å•åœºæ™¯

2. **å¯¹æ¯”å­¦ä¹ å¯èƒ½éš¾ä»¥å®ç°**
   - è´Ÿæ ·æœ¬æ„é€ å›°éš¾
   - éœ€è¦è¿åŠ¨å­¦æ¨¡å‹
   - **ç¼“è§£**: æš‚æ—¶ä¸åšå¯¹æ¯”å­¦ä¹ 

3. **å®éªŒæ—¶é—´å¯èƒ½å¾ˆé•¿**
   - éœ€è¦è®­ç»ƒå¤šä¸ª ablation
   - éœ€è¦æ”¶é›†å¤šä¸ªç¯å¢ƒçš„ play data
   - **ç¼“è§£**: å…ˆç”¨å°è§„æ¨¡ debug é…ç½®

---

## ä¸‹ä¸€æ­¥è¡ŒåŠ¨ ğŸš€

### ç«‹å³å¼€å§‹ (æœ¬å‘¨)
1. âœ… å®Œæˆæ¶æ„è®¾è®¡å’Œè®¡åˆ’æ–‡æ¡£
2. â³ æµ‹è¯•æ¨¡å‹åˆå§‹åŒ– (`test_alignment_expert_init.py`)
3. â³ ä¿®æ”¹ `PaliGemmaWithExpertModel.forward()`
4. â³ ä¿®æ”¹ `PI0Pytorch.forward()`

### çŸ­æœŸç›®æ ‡ (Week 1)
- å®Œæˆ Phase 1-2: åŸºç¡€æ¶æ„å’Œæ•°æ®æµ
- èƒ½å¤Ÿè®­ç»ƒä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¨¡å‹

### ä¸­æœŸç›®æ ‡ (Week 2-3)
- å®Œæˆ Phase 4-5: è®­ç»ƒé…ç½®å’Œè‡ªå¯¹é½
- æœ‰åˆæ­¥çš„å®éªŒç»“æœ

### é•¿æœŸç›®æ ‡ (Week 4+)
- å®Œæ•´çš„æ¶ˆèå®éªŒ
- é›¶æ ·æœ¬è¿ç§»éªŒè¯
- å‘å¸ƒå’Œæ–‡æ¡£

---

## å‚è€ƒèµ„æ–™ ğŸ“š

### ç›¸å…³è®ºæ–‡
1. **TTT Layers**: [Test-Time Training](https://arxiv.org/abs/...)
2. **Cross-Embodiment Transfer**: [Latent Space Alignment](https://arxiv.org/abs/...)
3. **RICL**: [Robot In-Context Learning](http://arxiv.org/abs/2508.02062)
4. **Scaling Proprioceptive-Visual Learning**: [Heterogeneous Pre-trained Transformers](https://...)

### ä»£ç å‚è€ƒ
- TTT Implementation: `src/openpi/models_pytorch/transformers_replace/models/gemma/ttt_with_gate.py`
- PI0 Model: `src/openpi/models_pytorch/pi0_pytorch.py`
- Training Loop: `src/openpi/training/train.py`

### å·²æœ‰æ–‡æ¡£
- `docs/analysis/gradient_checkpointing_analysis.md`
- `docs/analysis/attention_mask_analysis.md`
- `docs/analysis/TTT_Action_Expert_Integration_Plan.md`
- `docs/analysis/ttt_video_dit_comparison.md`

---

## æ›´æ–°æ—¥å¿— ğŸ“

### 2025-10-14
- âœ… åˆ›å»ºåˆå§‹è®¡åˆ’æ–‡æ¡£
- âœ… å®Œæˆæ¶æ„è®¾è®¡
- âœ… æ·»åŠ  alignment expert åˆ°ä»£ç 
- âœ… æ·»åŠ é…ç½®é€‰é¡¹
- âœ… è§„åˆ’è¯¦ç»†çš„å®ç°è·¯å¾„

---

**æœ€åæ›´æ–°**: 2025-10-14
**çŠ¶æ€**: Phase 1 è¿›è¡Œä¸­
**ä¸‹ä¸€é‡Œç¨‹ç¢‘**: å®ŒæˆåŸºç¡€æ¶æ„æµ‹è¯•
