# TTT Layer Integration - å…³é”®è®¾è®¡ç¬”è®°

> è®°å½• TTT (Test-Time Training) å±‚é›†æˆåˆ° Action Expert çš„å…³é”®è®¾è®¡å†³ç­–å’Œå®ç°ç»†èŠ‚

## ğŸ“‹ ç›®å½•
- [ä½¿ç”¨åœºæ™¯](#ä½¿ç”¨åœºæ™¯)
- [å…³é”®è®¾è®¡å†³ç­–](#å…³é”®è®¾è®¡å†³ç­–)
- [æ¶æ„ç»†èŠ‚](#æ¶æ„ç»†èŠ‚)
- [å®ç°è¦ç‚¹](#å®ç°è¦ç‚¹)
- [å‚è€ƒèµ„æ–™](#å‚è€ƒèµ„æ–™)

---

## ä½¿ç”¨åœºæ™¯

### Action Expert çš„ä½œç”¨
- **æ¨¡å‹**: `GemmaForCausalLM` (ä¸æ˜¯ PaliGemma)
- **ä»»åŠ¡**: å¯¹å™ªå£° Action è¿›è¡Œå»å™ª (denoising)
- **è¾“å…¥**: Noisy action sequences
- **å¤„ç†**: ä½¿ç”¨ TTT è¿›è¡Œ batch-level ä¼˜åŒ–

### ä¸ºä»€ä¹ˆéœ€è¦ TTT
- Action çš„å½¢çŠ¶æ˜¯ç¡®å®šçš„
- TTT ç”¨äº batch-level ä¼˜åŒ–ï¼Œ**ä¸æ˜¯ sequential çš„**
- æ¯ä¸ªæ ·æœ¬çš„æ‰€æœ‰ tokens ä¸€èµ·å‚ä¸ä¼˜åŒ–ï¼Œå»é™¤å™ªå£°

---

## å…³é”®è®¾è®¡å†³ç­–

### 1. TTT çš„æ”¾ç½®ä½ç½®

**å‚è€ƒè®ºæ–‡**: http://arxiv.org/abs/2504.05298

```
æ­£ç¡®çš„æ”¾ç½®é¡ºåº:
residual â†’ LayerNorm â†’ Attention â†’ TTT â†’ Residual Connection
                                    â†“
                         (TTT åœ¨ attention ä¹‹åï¼Œ
                          ä½†åœ¨ residual è¿æ¥ä¹‹å‰)
```

**å…³é”®å¼•ç”¨** (ç”¨æˆ·åŸè¯):
> "ttt å…·ä½“æ€ä¹ˆåŠ å…¥åˆ° attention ä¸­ å‚ç…§ä¸€ä¸‹ http://arxiv.org/abs/2504.05298ï¼Œttt åº”è¯¥æ˜¯åœ¨ attention ä¹‹åï¼Œä½†æ˜¯åœ¨ attention çš„ residual ä¹‹å‰"

### 2. Dual-Gate æœºåˆ¶

**å¿…é¡»ä½¿ç”¨ä¸¤ä¸ªç‹¬ç«‹çš„ gate**:
- `gate_attn`: æ§åˆ¶ attention åˆ†æ”¯
- `gate_ttt`: æ§åˆ¶ TTT åˆ†æ”¯

**æ¶æ„**:
```python
output = residual + gate_attn * attn_out + gate_ttt * ttt_out
```

**ç”¨æˆ·åŸè¯**:
> "æˆ‘è§‰å¾—è¿™é‡Œåº”è¯¥åˆ†ä¸¤ä¸ª Gateï¼Œgate_attn å’Œ gate_ttt"

### 3. Adaptive Normalization

**å…³é”®ä¿®æ­£** (ç”¨æˆ·åŸè¯):
> "å»æ‰ GemmaDecoderLayer ä¸­çš„ ttt ä¹‹åçš„ normï¼ŒæŠŠä»–çš„ norm é›†æˆåˆ° ttt layer è‡ªèº«é‡Œé¢"

**åŸå› **:
- TTT layer çš„ inner loop ä¼˜åŒ–ç›®æ ‡ä¹Ÿæœ‰æ®‹å·®è®¾è®¡ (å‚è§ `ttt.py` lines 503, 554, 556)
- normalization åº”è¯¥æ˜¯ TTT å†…éƒ¨çš„ä¸€éƒ¨åˆ†ï¼Œä¸åº”è¯¥åœ¨å¤–éƒ¨

**å®ç°**:
- TTT layer å†…éƒ¨ä½¿ç”¨ `GemmaRMSNorm`
- ä¸ `GemmaDecoderLayer` çš„å®ç°ä¿æŒç»Ÿä¸€
- `adarms_cond` åŒæ—¶å½±å“ norm å’Œ gate

---

## æ¶æ„ç»†èŠ‚

### TTT ä¼˜åŒ–æ¨¡å¼

**ä¸åŒäºæ ‡å‡† TTT**:

| ç‰¹æ€§ | æ ‡å‡† TTT (ttt.py) | Action Expert TTT |
|------|------------------|-------------------|
| Position IDs | âœ… éœ€è¦ | âŒ ä¸éœ€è¦ |
| RoPE | âœ… ä½¿ç”¨ | âŒ ä¸ä½¿ç”¨ |
| Sequential Scan | âœ… ä½¿ç”¨ | âŒ ä¸ä½¿ç”¨ |
| Mini-batch åˆ†æ®µ | âœ… åˆ†æ®µå¤„ç† | âŒ æ•´ä¸ªåºåˆ—ä¸€èµ· |
| ä¼˜åŒ–æ–¹å¼ | Position-wise | Batch-parallel |

**ç”¨æˆ·åŸè¯**:
> "æˆ‘è¿™é‡Œå…¶å®ä¸ç”¨ cache çš„ï¼Œå› ä¸ºè¿™é‡Œçš„ ttt æ˜¯ batch level çš„ä¼˜åŒ–ï¼Œè€Œä¸æ˜¯ sequential çš„"
>
> "æ‰€ä»¥è¿™ä¸ª TTT çš„è¿è¡Œï¼Œè¿™ä¸ª TTT çš„ä¼˜åŒ–æ˜¯åœ¨æ‰€æœ‰çš„å™ªå£° Action Token ä¸Šè¿›è¡Œçš„"
>
> "æ‰€ä»¥è¿™é‡Œçš„ Position IDS å¯ä»¥å»æ‰ï¼Œä¸éœ€è¦è¿™ä¸ªå‚æ•°"

### Batch-Parallel ä¼˜åŒ–

**æ ¸å¿ƒæ¦‚å¿µ**:
```python
# æ¯ä¸ª batch æ ·æœ¬ç»´æŠ¤ç‹¬ç«‹çš„ W å‚æ•°
W1_init = self.W1.unsqueeze(0).expand(B, -1, -1, -1).clone()  # [B, num_heads, head_dim, head_dim]

# æ‰€æœ‰ tokens åŒæ—¶å‚ä¸ä¼˜åŒ–
grad_W1 = torch.einsum("bhld,bhlf->bhdf", X1, grad_l_wrt_Z1)
W1_updated = W1_init - eta * grad_W1
```

**å…³é”®ç‚¹**:
- **Batch ç»´åº¦**: ä¸åŒæ ·æœ¬æœ‰ä¸åŒçš„ W (ç”¨äºå»å™ªä¸åŒçš„ noisy action)
- **Sequence ç»´åº¦**: åŒä¸€æ ·æœ¬çš„æ‰€æœ‰ tokens å…±äº«ä¸€ä¸ª Wï¼Œä¸€èµ·å‚ä¸ä¼˜åŒ–
- **æ— ä½ç½®ä¾èµ–**: æ‰€æœ‰ tokens å¹³ç­‰å¯¹å¾…

**ç”¨æˆ·åŸè¯**:
> "æ³¨æ„åœ¨è¿™é‡Œé‡Œ mini_batch_size çš„å¤§å°ç­‰äº seq_lenï¼Œæ‰€ä»¥è¿™é‡Œä¸å­˜åœ¨æ²¿ç€ sequence æ‰«æçš„æƒ…å†µ"

---

## å®ç°è¦ç‚¹

### 1. æ–‡ä»¶ç»“æ„

```
src/openpi/models_pytorch/
â”œâ”€â”€ ttt_with_gate.py                    # æ–°æ–‡ä»¶ï¼šTTT layer å®ç°
â””â”€â”€ transformers_replace/models/gemma/
    â”œâ”€â”€ configuration_gemma.py          # æ·»åŠ  TTT é…ç½®
    â””â”€â”€ modeling_gemma.py               # é›†æˆ TTT åˆ° decoder layer
```

### 2. GemmaRMSNorm ç»Ÿä¸€

**ç”¨æˆ·è¦æ±‚**:
> "æ˜¯çš„ï¼Œä¹Ÿæ¢æˆ GemmaRMSNorm è·Ÿ GemmaDecoderLayer é‡Œé¢çš„å®ç°ç»Ÿä¸€"

**å®ç°**:
- TTT layer å†…éƒ¨ç›´æ¥ä½¿ç”¨ `GemmaRMSNorm`
- ä¿æŒä¸ `GemmaDecoderLayer` å®Œå…¨ä¸€è‡´çš„ adaptive normalization è¡Œä¸º
- `adarms_cond` å½±å“ scaleã€shift å’Œ gate

### 3. Gate çš„å½¢çŠ¶

**å…³é”®**: Gate çš„å½¢çŠ¶æ˜¯ `[B, 1, hidden_size]`ï¼Œä¸æ˜¯ `[B, 1]`

**åŸå› **:
```python
# GemmaRMSNorm.forward è¿”å›:
# - normed_inputs: [B, L, hidden_size]
# - gate: [B, 1, hidden_size]  (ç»è¿‡ unsqueeze(1) åçš„ modulation chunk)

# åœ¨ residual connection ä¸­ä½¿ç”¨:
hidden_states = hidden_states + gate_ttt * ttt_output
# gate_ttt: [B, 1, hidden_size] broadcasts with ttt_output: [B, L, hidden_size]
```

### 4. ä¸è¦ä¿®æ”¹åŸå§‹ ttt.py

**ç”¨æˆ·æ˜ç¡®æŒ‡ç¤º**:
> "ä¸è¦ç›´æ¥ä¿®æ”¹ @ttt.py å°†å…¶ä½œä¸ºå‚ç…§ï¼Œä½ å¯ä»¥åœ¨ /opt/tiger/openpi/src/openpi/models_pytorch è¿™ä¸ªé‡Œé¢æ–°å»ºç±»"
>
> "å¹¶ä¸åªæ˜¯ wrapper è€Œæ˜¯ ttt layer æœ¬èº«ï¼Œä½ å¯ä»¥å‚ç…§ @ttt.py æˆ–è€…å¤åˆ¶è¿‡æ¥éƒ½è¡Œ"

**æ‰§è¡Œæ–¹å¼**:
- åˆ›å»ºæ–°æ–‡ä»¶ `ttt_with_gate.py`
- å¤åˆ¶å¹¶æ”¹é€  `TTTLinear` çš„æ ¸å¿ƒé€»è¾‘
- ä¿æŒåŸå§‹ `ttt.py` ä¸å˜ä½œä¸ºå‚è€ƒ

---

## é…ç½®å‚æ•°

### GemmaConfig æ–°å¢å‚æ•°

```python
use_ttt: bool = False                      # æ˜¯å¦å¯ç”¨ TTT
ttt_mini_batch_size: int = 64              # å…¼å®¹å‚æ•°ï¼ˆå®é™…ä¸ä½¿ç”¨ï¼‰
ttt_mode: str = "after_attn"               # é›†æˆæ¨¡å¼ï¼ˆå…¼å®¹å‚æ•°ï¼‰
ttt_layer_positions: Optional[list] = None # æŒ‡å®šå“ªäº›å±‚ä½¿ç”¨ TTT
```

**ç”¨æˆ·åŸè¯**:
> "ttt_layer_positions ttt_mode éƒ½åŠ ä¸Š"

### TTTWithAdaptiveNorm å‚æ•°

```python
TTTWithAdaptiveNorm(
    num_heads=config.num_attention_heads,
    hidden_size=config.hidden_size,
    mini_batch_size=64,           # ä¿ç•™å…¼å®¹æ€§ï¼Œå®é™…ä¸ä½¿ç”¨
    rope_theta=config.rope_theta, # ä¿ç•™å…¼å®¹æ€§ï¼Œå®é™…ä¸ä½¿ç”¨
    use_adarms=True,              # æ˜¯å¦ä½¿ç”¨ adaptive normalization
    adarms_cond_dim=cond_dim,     # Timestep embedding ç»´åº¦
    eps=config.rms_norm_eps
)
```

---

## ä»£ç å…³é”®ç‚¹

### TTT Layer Forward Flow

```python
def forward(self, hidden_states, adarms_cond=None, cache_params=None):
    # 1. Adaptive RMS Norm (è¿”å› normalized å’Œ gate)
    normalized_hidden_states, gate = self.input_norm(hidden_states, adarms_cond)

    # 2. Q/K/V projections
    XQ, XK, XV = self.get_qkv_projections(normalized_hidden_states)

    # 3. Batch-parallel TTT optimization
    output = self.ttt_batch_parallel(XQ, XK, XV, normalized_hidden_states)

    # 4. Post-norm and projection
    output = self.post_norm(output)
    output = self.o_proj(output)

    return output, gate  # gate: [B, 1, hidden_size]
```

### GemmaDecoderLayer Integration

```python
# [1] Attention + TTT Block
residual = hidden_states
hidden_states, gate_attn = self.input_layernorm(hidden_states, adarms_cond)

# Attention
attn_output, _ = self.self_attn(hidden_states, ...)

# Gated residual for attention
hidden_states = _gated_residual(residual, attn_output, gate_attn)

# TTT (if enabled)
if self.ttt_layer is not None:
    ttt_output, gate_ttt = self.ttt_layer(attn_output, adarms_cond)
    if gate_ttt is not None:
        hidden_states = hidden_states + gate_ttt * ttt_output
    else:
        hidden_states = hidden_states + ttt_output

# [2] MLP Block
residual = hidden_states
hidden_states, gate_mlp = self.post_attention_layernorm(hidden_states, adarms_cond)
hidden_states = self.mlp(hidden_states)
hidden_states = _gated_residual(residual, hidden_states, gate_mlp)
```

### TTT Batch-Parallel Optimization

```python
def ttt_batch_parallel(self, XQ, XK, XV, X):
    # æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹çš„ W å‚æ•°
    W1_init = self.W1.unsqueeze(0).expand(B, -1, -1, -1).clone()
    b1_init = self.b1.unsqueeze(0).expand(B, -1, -1, -1).clone()

    # è®¡ç®—å­¦ä¹ ç‡ (åŸºäº X çš„å¹³å‡å€¼)
    X_mean = X.mean(dim=1)
    eta = self.ttt_base_lr * ttt_lr / head_dim

    # TTT ä¼˜åŒ–ç›®æ ‡: æœ€å°åŒ–é‡æ„è¯¯å·®
    Z1 = torch.einsum("bhld,bhdf->bhlf", XK, W1_init) + b1_init
    reconstruction_target = XV - XK  # æ®‹å·®è®¾è®¡

    # è®¡ç®—æ¢¯åº¦
    grad_l_wrt_Z1 = ln_fused_l2_bwd(Z1, reconstruction_target, ln_weight, ln_bias)

    # Batch-parallel æ¢¯åº¦ä¸‹é™
    grad_W1 = torch.einsum("bhld,bhlf->bhdf", XK, grad_l_wrt_Z1)
    grad_b1 = grad_l_wrt_Z1.sum(dim=2, keepdim=True)

    W1_updated = W1_init - eta * grad_W1
    b1_updated = b1_init - eta * grad_b1

    # ä½¿ç”¨æ›´æ–°åçš„å‚æ•°å‰å‘ä¼ æ’­
    Z1_updated = torch.einsum("bhld,bhdf->bhlf", XQ, W1_updated) + b1_updated
    Z1_normalized = ln_fwd(Z1_updated, ln_weight, ln_bias)

    return XQ + Z1_normalized  # æ®‹å·®è¿æ¥
```

---

## å‚è€ƒèµ„æ–™

### è®ºæ–‡
- **TTT Paper**: http://arxiv.org/abs/2504.05298
  - æè¿°äº† TTT å¦‚ä½•åŠ å…¥åˆ° attention ä¸­
  - TTT åº”è¯¥åœ¨ attention ä¹‹åï¼Œä½†åœ¨ residual ä¹‹å‰

### ä»£ç å‚è€ƒ
- **Original TTT**: `/opt/tiger/openpi/ttt.py`
  - Lines 234-239: TTT å†…éƒ¨ LayerNorm åˆå§‹åŒ–
  - Line 503: é‡æ„ç›®æ ‡çš„æ®‹å·®è®¾è®¡ `reconstruction_target = XV - XK`
  - Lines 554, 556: è¾“å‡ºçš„æ®‹å·®è¿æ¥ `XQW = XQ + Z1_bar`

### ç›¸å…³æ–‡ä»¶
- `configuration_gemma.py`: TTT é…ç½®å‚æ•°
- `modeling_gemma.py`: TTT é›†æˆåˆ° decoder layer
- `ttt_with_gate.py`: TTT layer å®ç°
- `debug_action_expert.py`: æµ‹è¯•è„šæœ¬

---

## æµ‹è¯•è¦ç‚¹

### æµ‹è¯•åœºæ™¯

1. **æ—  AdaRMS æ¨¡å¼**
   - `use_adarms=False`
   - gate åº”è¯¥ä¸º `None`
   - TTT æ­£å¸¸å·¥ä½œ

2. **æœ‰ AdaRMS æ¨¡å¼**
   - `use_adarms=True`
   - gate å½¢çŠ¶: `[B, 1, hidden_size]`
   - gate åˆå§‹å€¼æ¥è¿‘ 0 (å› ä¸º dense layer åˆå§‹åŒ–ä¸º 0)

3. **æ¢¯åº¦æµæµ‹è¯•**
   - éªŒè¯æ¢¯åº¦å¯ä»¥æ­£ç¡®åå‘ä¼ æ’­
   - `adarms_cond` çš„æ¢¯åº¦åº”è¯¥æ­£å¸¸

4. **ä¸åŒåºåˆ—é•¿åº¦**
   - æµ‹è¯• seq_len = 8, 16, 32, 64
   - æ‰€æœ‰é•¿åº¦éƒ½åº”è¯¥æ­£å¸¸å·¥ä½œ

### è¿è¡Œæµ‹è¯•

```bash
# TTT ç‹¬ç«‹æµ‹è¯•
python src/openpi/models_pytorch/ttt_with_gate.py

# Action Expert é›†æˆæµ‹è¯•
python debug_action_expert.py
```

---

## å¸¸è§é—®é¢˜

### Q: ä¸ºä»€ä¹ˆä¸éœ€è¦ position_idsï¼Ÿ
**A**: å› ä¸ºè¿™é‡Œçš„ TTT æ˜¯ç”¨äºå»å™ªï¼Œä¸æ˜¯ sequential generationã€‚æ‰€æœ‰ tokens å¹³ç­‰å¯¹å¾…ï¼Œåœ¨æ•´ä¸ªåºåˆ—ä¸Šè¿›è¡Œ batch-level ä¼˜åŒ–ã€‚

### Q: ä¸ºä»€ä¹ˆä¸éœ€è¦ scanï¼Ÿ
**A**: å› ä¸º `mini_batch_size == seq_len`ï¼Œæ•´ä¸ªåºåˆ—å°±æ˜¯ä¸€ä¸ª mini-batchï¼Œä¸éœ€è¦åˆ†æ®µå¤„ç†ã€‚

### Q: ä¸ºä»€ä¹ˆéœ€è¦ä¸¤ä¸ª gateï¼Ÿ
**A**: å› ä¸º attention å’Œ TTT æ˜¯ä¸¤ä¸ªç‹¬ç«‹çš„åˆ†æ”¯ï¼Œéœ€è¦ç‹¬ç«‹æ§åˆ¶å®ƒä»¬çš„è´¡çŒ®ã€‚è¿™æ ·æ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°åœ¨ä¸åŒçš„ timestep ä¸‹å¦‚ä½•å¹³è¡¡ä¸¤ä¸ªåˆ†æ”¯ã€‚

### Q: gate çš„å½¢çŠ¶ä¸ºä»€ä¹ˆæ˜¯ [B, 1, hidden_size]ï¼Ÿ
**A**: è¿™æ˜¯ `GemmaRMSNorm` çš„è®¾è®¡ã€‚modulation ç»è¿‡ `unsqueeze(1)` å chunk æˆ scaleã€shiftã€gateï¼Œæ¯ä¸ªéƒ½æ˜¯ `[B, 1, hidden_size]`ï¼Œå¯ä»¥å’Œ `[B, L, hidden_size]` æ­£ç¡® broadcastã€‚

### Q: adarms_cond å½±å“ä»€ä¹ˆï¼Ÿ
**A**:
- **Normalization**: é€šè¿‡ scale å’Œ shift è°ƒæ•´å½’ä¸€åŒ–ç»“æœ
- **Gate**: æ§åˆ¶åˆ†æ”¯çš„è´¡çŒ®ç¨‹åº¦
- ä¸¤è€…éƒ½ä¾èµ–äº diffusion timestep embedding

---

## Git Commit

**Commit Hash**: `a14a247`

**Title**: Add TTT (Test-Time Training) layer integration to Action Expert

**Files Changed**:
- `configuration_gemma.py` (+16 lines)
- `modeling_gemma.py` (+59 lines, -6 lines)
- `ttt_with_gate.py` (+419 lines, new file)
- `.gitignore` (+1 line)

**Total**: 489 insertions, 6 deletions

---

_Last Updated: 2025-10-09_
_Author: ç”¨æˆ·æŒ‡å¯¼ + Claude Code å®ç°_
